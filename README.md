# AI Server Deployment with Ollama

This project involved deploying and configuring a self-hosted AI server using Ollama on a Linux-based VPS. I tested local LLM inference with GPU acceleration, optimized performance, and ran multiple models.

## Tools & Tech
- Linux (Debian VPS)
- Ollama
- Deepseek
- GPU acceleration
- Terminal / CLI

## Key Tasks
- Installed Ollama on a VPS
- Configured GPU drivers and environment
- Ran local inference without cloud APIs
- Benchmarked different model sizes
- Tuned resource usage and model responsiveness

## Skills Demonstrated
- LLM architecture & deployment
- Linux server administration
- AI toolchain integration
