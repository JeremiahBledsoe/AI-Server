# AI_Server
LLM inference setup using Ollama
